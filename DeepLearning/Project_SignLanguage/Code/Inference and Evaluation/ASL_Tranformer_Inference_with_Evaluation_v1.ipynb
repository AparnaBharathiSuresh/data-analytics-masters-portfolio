{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc37004-7280-41cf-9477-619ed12072b3",
      "metadata": {
        "id": "dfc37004-7280-41cf-9477-619ed12072b3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "#from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff3aa60-024e-45f1-b247-472051035630",
      "metadata": {
        "id": "2ff3aa60-024e-45f1-b247-472051035630"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db62a41-9c81-48fd-a8ce-7dd7303c54ab",
      "metadata": {
        "id": "6db62a41-9c81-48fd-a8ce-7dd7303c54ab"
      },
      "outputs": [],
      "source": [
        "# === Load class ID to word mapping ===\n",
        "with open(\"sign_to_prediction_index_map.json\", \"r\") as f:\n",
        "    word_to_class = json.load(f)\n",
        "class_to_word = {v: k for k, v in word_to_class.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6865cd-ad69-4269-a7f0-021010581bfb",
      "metadata": {
        "id": "ee6865cd-ad69-4269-a7f0-021010581bfb"
      },
      "source": [
        "## === Step 1: Rebuild the model ==="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "405938ce-7948-4af4-8fbc-e1bd54ec4f5a",
      "metadata": {
        "id": "405938ce-7948-4af4-8fbc-e1bd54ec4f5a"
      },
      "source": [
        "# Define the Model class\n",
        "\n",
        "# from model import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92f5dec-f838-4d1d-b72c-b6cc17c48b9b",
      "metadata": {
        "id": "b92f5dec-f838-4d1d-b72c-b6cc17c48b9b"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,num_embed, d_model, max_len, n_heads, num_encoders, num_classes, dropout, activation, batch_first = True):\n",
        "        super(Model,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.n_heads = n_heads\n",
        "        self.num_encoders = num_encoders\n",
        "        self.dropout = dropout\n",
        "        self.num_embed = num_embed\n",
        "\n",
        "        self.embed = nn.ModuleList()\n",
        "\n",
        "        self.embed.append(nn.Linear(d_model, d_model*2, bias = True))\n",
        "        self.embed.append(nn.LayerNorm(d_model*2))\n",
        "        self.embed.append(torch.nn.ReLU(inplace = True))\n",
        "        for i in range(num_embed-2):\n",
        "            self.embed.append(torch.nn.Linear(d_model*2, d_model*2,bias = True))\n",
        "            self.embed.append(nn.LayerNorm(d_model*2))\n",
        "            self.embed.append(torch.nn.ReLU(inplace = True))\n",
        "        self.embed.append(nn.Linear(d_model*2, d_model, bias = True))\n",
        "        self.embed.append(nn.LayerNorm(d_model))\n",
        "        self.embed.append(torch.nn.ReLU(inplace = True))\n",
        "\n",
        "\n",
        "        self.positionalEncoder = PositionalEncoding(d_model = d_model, max_len = max_len)\n",
        "        self.cls_embedding = nn.Parameter(torch.zeros((1, d_model)))\n",
        "        self.encoderLayer = nn.TransformerEncoderLayer(d_model = self.d_model,\n",
        "                                                       nhead = self.n_heads,\n",
        "                                                       dim_feedforward=self.d_model*2,\n",
        "                                                      dropout = self.dropout,\n",
        "                                                      activation = activation,\n",
        "                                                      batch_first = batch_first)\n",
        "        self.Encoder = nn.TransformerEncoder(encoder_layer = self.encoderLayer, num_layers = self.num_encoders)\n",
        "        self.output = nn.Linear(self.d_model, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.embed:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.positionalEncoder(x)\n",
        "        x = x + self.cls_embedding\n",
        "        x = self.Encoder(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "393b83d6-ca29-4eb9-8edc-dcaf8a4b5f0b",
      "metadata": {
        "id": "393b83d6-ca29-4eb9-8edc-dcaf8a4b5f0b"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 135):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :,  0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :,  1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe\n",
        "        return self.dropout(x).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fb6f27-22d9-48ef-a204-609166d0cd9c",
      "metadata": {
        "id": "b4fb6f27-22d9-48ef-a204-609166d0cd9c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "BEST_PARAMS = {\n",
        "    \"num_embed_layers\": 4 ,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_encoder_layers\": 2,\n",
        "    \"dropout\": 0.11073790254354612\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607a04bb-df3c-48d9-b1b7-b5e715201999",
      "metadata": {
        "id": "607a04bb-df3c-48d9-b1b7-b5e715201999"
      },
      "outputs": [],
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d1db57-17db-4fff-9e07-fb21cc3f196f",
      "metadata": {
        "id": "75d1db57-17db-4fff-9e07-fb21cc3f196f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Create model instance\n",
        "model = Model(\n",
        "    num_embed=BEST_PARAMS[\"num_embed_layers\"],\n",
        "    d_model=176,  # <--- must match the saved model\n",
        "    max_len=135,\n",
        "    n_heads=BEST_PARAMS[\"n_heads\"],\n",
        "    num_encoders=BEST_PARAMS[\"n_encoder_layers\"],\n",
        "    num_classes=250,\n",
        "    dropout=BEST_PARAMS[\"dropout\"],\n",
        "    activation='relu',\n",
        "    batch_first=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104e575f-af01-4274-8671-70c46f320b37",
      "metadata": {
        "id": "104e575f-af01-4274-8671-70c46f320b37"
      },
      "source": [
        "## === Step 2: Load the saved model weights ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7522fe-115f-472d-860e-4b48ccda0258",
      "metadata": {
        "id": "2c7522fe-115f-472d-860e-4b48ccda0258",
        "outputId": "c5506785-a93c-4c45-cc8e-cfa82f18c05c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_812249/15744313.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"best_model_final.pth\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embed): ModuleList(\n",
              "    (0): Linear(in_features=176, out_features=352, bias=True)\n",
              "    (1): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=352, out_features=352, bias=True)\n",
              "    (4): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=352, out_features=352, bias=True)\n",
              "    (7): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Linear(in_features=352, out_features=176, bias=True)\n",
              "    (10): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "  )\n",
              "  (positionalEncoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoderLayer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=176, out_features=176, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=176, out_features=352, bias=True)\n",
              "    (dropout): Dropout(p=0.11073790254354612, inplace=False)\n",
              "    (linear2): Linear(in_features=352, out_features=176, bias=True)\n",
              "    (norm1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.11073790254354612, inplace=False)\n",
              "    (dropout2): Dropout(p=0.11073790254354612, inplace=False)\n",
              "  )\n",
              "  (Encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=176, out_features=176, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=176, out_features=352, bias=True)\n",
              "        (dropout): Dropout(p=0.11073790254354612, inplace=False)\n",
              "        (linear2): Linear(in_features=352, out_features=176, bias=True)\n",
              "        (norm1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.11073790254354612, inplace=False)\n",
              "        (dropout2): Dropout(p=0.11073790254354612, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): Linear(in_features=176, out_features=250, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dict = torch.load(\"best_model_final.pth\")\n",
        "filtered_state_dict = {\n",
        "    k: v for k, v in state_dict.items()\n",
        "    if k in model.state_dict() and model.state_dict()[k].shape == v.shape\n",
        "}\n",
        "model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d287cb-6896-45ca-a49f-b01d31f69548",
      "metadata": {
        "id": "99d287cb-6896-45ca-a49f-b01d31f69548"
      },
      "source": [
        "## === Step 3: Prepare a single input ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a709517-13c9-4abf-9b72-ba72b761a802",
      "metadata": {
        "id": "1a709517-13c9-4abf-9b72-ba72b761a802"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "683f8a83-d860-4f8d-9a8a-70991a7fc3db",
      "metadata": {
        "id": "683f8a83-d860-4f8d-9a8a-70991a7fc3db",
        "outputId": "10913f2d-a065-4305-b61b-fc1bc188ea23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "['train_landmark_files/28656/3311214787.parquet']\n"
          ]
        }
      ],
      "source": [
        "print(type(X_test))\n",
        "print(type(X_test[0]))  # Check the type of the first element\n",
        "print(X_test[0])  # Inspect the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab03fa50",
      "metadata": {
        "id": "ab03fa50",
        "outputId": "a873e394-5819-4d0e-8da9-40fcd21c25fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 15984\n",
            "-rw-r--r--  1 vscode vscode   23940 Apr 27 12:55  ASL_Tranformer_Inference_April25_1.ipynb\n",
            "-rw-r--r--  1 vscode vscode   58060 Apr 27 15:29 'ASL_Tranformer_Inference_with Evaluation_April26_1.ipynb'\n",
            "-rw-r--r--  1 vscode vscode  450517 Apr 27 15:28  X_test.npy\n",
            "-rw-r--r--  1 vscode vscode 4053085 Apr 27 15:28  X_train.npy\n",
            "-rw-r--r--  1 vscode vscode 5795946 Apr 27 12:54  best_model_final.pth\n",
            "-rw-r--r--  1 vscode vscode 5205410 Apr 27 12:52  final_train.csv\n",
            "-rw-r--r--  1 vscode vscode    3352 Apr 27 12:52  sign_to_prediction_index_map.json\n",
            "drwxr-xr-x 23 vscode vscode    4096 Apr 27 12:54  \u001b[0m\u001b[01;34mtrain_landmark_files\u001b[0m/\n",
            "-rw-r--r--  1 vscode vscode   75712 Apr 27 15:29  y_test.npy\n",
            "-rw-r--r--  1 vscode vscode  680360 Apr 27 15:29  y_train.npy\n"
          ]
        }
      ],
      "source": [
        "ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e26ee4-4aa4-4827-a971-b06a71def10b",
      "metadata": {
        "id": "99e26ee4-4aa4-4827-a971-b06a71def10b",
        "outputId": "e97ac11b-f3e4-4745-a7ba-2686d4b2e0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    x0   x1   x2   x3   x4   x5   x6   x7   x8   x9  ...  y78  y79  y80  y81  \\\n",
            "0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0 -1.0   \n",
            "1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0 -1.0   \n",
            "2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0 -1.0   \n",
            "3 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0 -1.0   \n",
            "4 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0 -1.0   \n",
            "\n",
            "   y82  y83  y84  y85  y86  y87  \n",
            "0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
            "1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
            "2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
            "3 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
            "4 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
            "\n",
            "[5 rows x 176 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the parquet file for \"Hello\" Sign\n",
        "\n",
        "parquet_data = pd.read_parquet('train_landmark_files/55372/1458242525.parquet')\n",
        "\n",
        "#parquet_data = pd.read_parquet('train_landmark_files/28656/1460359.parquet')\n",
        "#\"C:\\Users\\aryama\\Desktop\\Aryama_data255_April21\\train_landmark_files\\28656\\1460359.parquet\"\n",
        "# Check the contents of the parquet file\n",
        "print(parquet_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32f083a-2201-4fd9-951d-19c386be4c7e",
      "metadata": {
        "id": "e32f083a-2201-4fd9-951d-19c386be4c7e"
      },
      "outputs": [],
      "source": [
        "# If the data contains numerical columns, extract them as a numpy array\n",
        "data_array = parquet_data.to_numpy()\n",
        "\n",
        "# Pick a single sample and convert to tensor\n",
        "single_sample = data_array[0]  # assuming you're using the first row\n",
        "single_sample_tensor = torch.tensor(single_sample, dtype=torch.float32).to(device)\n",
        "single_sample_tensor = single_sample_tensor.unsqueeze(0)  # Shape: (1, 135, D)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86896129-6705-410b-80d3-33a92db5470b",
      "metadata": {
        "id": "86896129-6705-410b-80d3-33a92db5470b"
      },
      "source": [
        "\n",
        "### === Step 4: Inference ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0ad997-88f0-44d8-aca7-d7c9223ca3df",
      "metadata": {
        "id": "fe0ad997-88f0-44d8-aca7-d7c9223ca3df",
        "outputId": "7e7d9f77-3f23-4515-fd0a-dbebd993a6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class for sample 0: 105\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'device' is already set to either 'cpu' or 'cuda'\n",
        "model = model.to(device)  # Make sure the model is on the same device as the input\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(single_sample_tensor)  # Now the model and input tensor are on the same device\n",
        "    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "print(f\"Predicted class for sample 0: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395b3d3d-f373-4f55-a825-94fffe11df00",
      "metadata": {
        "id": "395b3d3d-f373-4f55-a825-94fffe11df00"
      },
      "source": [
        "###  Predict Top-5 classes ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543a96c8-ece5-4aff-97f5-24c96f021f21",
      "metadata": {
        "id": "543a96c8-ece5-4aff-97f5-24c96f021f21",
        "outputId": "8a921d4f-7f79-44bb-c26c-a41ecc6646d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-5 predicted classes: [105  44  80 208 175]\n",
            "Top-5 probabilities: [0.3786606  0.21133378 0.1447588  0.14234644 0.12290039]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(single_sample_tensor)\n",
        "    top5_prob, top5_labels = torch.topk(output, 5, dim=1)\n",
        "\n",
        "print(f\"Top-5 predicted classes: {top5_labels.cpu().numpy()[0]}\")\n",
        "print(f\"Top-5 probabilities: {top5_prob.softmax(dim=1).cpu().numpy()[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f9163e-2415-44c2-8bf4-0aab3e72a0c7",
      "metadata": {
        "id": "b8f9163e-2415-44c2-8bf4-0aab3e72a0c7"
      },
      "source": [
        "### Sentence Level Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a494fb5-014f-4527-a63d-f9e61fd1fc4e",
      "metadata": {
        "id": "7a494fb5-014f-4527-a63d-f9e61fd1fc4e"
      },
      "outputs": [],
      "source": [
        "## pip install transformers\n",
        "#pip install git+https://github.com/huggingface/transformers\n",
        "#git clone https://github.com/huggingface/transformers.git\n",
        "#cd transformers\n",
        "#pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810a3182-9d86-4947-b3fe-c027c542bb67",
      "metadata": {
        "id": "810a3182-9d86-4947-b3fe-c027c542bb67"
      },
      "outputs": [],
      "source": [
        "# === Step 1: Load necessary components ===\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assume Model class is already defined elsewhere\n",
        "# from model import Model\n",
        "\n",
        "# Example: best hyperparameters (update if needed)\n",
        "BEST_PARAMS = {\n",
        "    \"num_embed_layers\": 4,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_encoder_layers\": 2,\n",
        "    \"dropout\": 0.11\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160cd255-2c9e-4a63-b2c1-22a99cf2c66e",
      "metadata": {
        "id": "160cd255-2c9e-4a63-b2c1-22a99cf2c66e"
      },
      "outputs": [],
      "source": [
        "sign_model = Model(\n",
        "    num_embed=BEST_PARAMS[\"num_embed_layers\"],\n",
        "    d_model=176,\n",
        "    max_len=135,\n",
        "    n_heads=BEST_PARAMS[\"n_heads\"],\n",
        "    num_encoders=BEST_PARAMS[\"n_encoder_layers\"],\n",
        "    num_classes=250,\n",
        "    dropout=BEST_PARAMS[\"dropout\"],\n",
        "    activation='relu',\n",
        "    batch_first=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880213df-3b29-4024-8d9f-b6c1703d9215",
      "metadata": {
        "id": "880213df-3b29-4024-8d9f-b6c1703d9215",
        "outputId": "bd999b7f-af2a-4480-cace-d96f30ae246b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_812249/3842101145.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"best_model_final.pth\", map_location=device)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embed): ModuleList(\n",
              "    (0): Linear(in_features=176, out_features=352, bias=True)\n",
              "    (1): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=352, out_features=352, bias=True)\n",
              "    (4): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=352, out_features=352, bias=True)\n",
              "    (7): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Linear(in_features=352, out_features=176, bias=True)\n",
              "    (10): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "  )\n",
              "  (positionalEncoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoderLayer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=176, out_features=176, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=176, out_features=352, bias=True)\n",
              "    (dropout): Dropout(p=0.11, inplace=False)\n",
              "    (linear2): Linear(in_features=352, out_features=176, bias=True)\n",
              "    (norm1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.11, inplace=False)\n",
              "    (dropout2): Dropout(p=0.11, inplace=False)\n",
              "  )\n",
              "  (Encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=176, out_features=176, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=176, out_features=352, bias=True)\n",
              "        (dropout): Dropout(p=0.11, inplace=False)\n",
              "        (linear2): Linear(in_features=352, out_features=176, bias=True)\n",
              "        (norm1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.11, inplace=False)\n",
              "        (dropout2): Dropout(p=0.11, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): Linear(in_features=176, out_features=250, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dict = torch.load(\"best_model_final.pth\", map_location=device)\n",
        "filtered_state_dict = {\n",
        "    k: v for k, v in state_dict.items()\n",
        "    if k in sign_model.state_dict() and sign_model.state_dict()[k].shape == v.shape\n",
        "}\n",
        "sign_model.load_state_dict(filtered_state_dict, strict=False)\n",
        "sign_model = sign_model.to(device)\n",
        "sign_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f819280-6656-494c-b819-90f7a12730f5",
      "metadata": {
        "id": "1f819280-6656-494c-b819-90f7a12730f5",
        "outputId": "d390f56f-48c9-4049-c836-54740889b5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106b7391-4146-42b7-9cc1-2fb0850c7ffa",
      "metadata": {
        "id": "106b7391-4146-42b7-9cc1-2fb0850c7ffa"
      },
      "outputs": [],
      "source": [
        "# === Step 6.1: Load test data (file paths) ===\n",
        "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4388917-c4df-46fb-a868-afb8db8fb156",
      "metadata": {
        "id": "c4388917-c4df-46fb-a868-afb8db8fb156",
        "outputId": "a3d8dbad-749b-4e5f-eb0a-3435cd164dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 9166: Predicted unique word: fast\n",
            "Sample 1747: Predicted unique word: milk\n",
            "Sample 7219: Predicted unique word: empty\n",
            "Sample 1085: Predicted unique word: aunt\n",
            "\n",
            "Final 5 unique predicted words: ['fast', 'milk', 'empty', 'aunt']\n"
          ]
        }
      ],
      "source": [
        "# === Step 7: Inference loop ===\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "predicted_words = set()  # use a set to auto-remove duplicates\n",
        "attempts = 0\n",
        "max_attempts = 100  # prevent infinite loop\n",
        "\n",
        "# === Step 7: Inference loop ===\n",
        "while len(predicted_words) < 5 and attempts < max_attempts:\n",
        "    idx = random.choice(range(len(X_test)))  # pick random sample\n",
        "    file_path = X_test[idx][0]\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # Correct: don't flatten\n",
        "    sample_features = df.values  # shape (135, 176)\n",
        "    sample_tensor = torch.tensor(sample_features, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 135, 176)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = sign_model(sample_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    predicted_word = class_to_word.get(predicted_class, \"unknown\")\n",
        "\n",
        "    if predicted_word != \"unknown\":\n",
        "        if predicted_word not in predicted_words:\n",
        "            print(f\"Sample {idx}: Predicted unique word: {predicted_word}\")\n",
        "        predicted_words.add(predicted_word)\n",
        "\n",
        "    attempts += 1  # avoid infinite loops if not enough unique words\n",
        "\n",
        "# Finally, convert set to list\n",
        "predicted_words = list(predicted_words)\n",
        "\n",
        "print(\"\\nFinal 5 unique predicted words:\", predicted_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f11c030-0db0-46a7-b8bf-55282e5887a4",
      "metadata": {
        "id": "6f11c030-0db0-46a7-b8bf-55282e5887a4"
      },
      "source": [
        "### Correct word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ec5c9c-1703-4ec7-b903-8110db15ba6e",
      "metadata": {
        "id": "63ec5c9c-1703-4ec7-b903-8110db15ba6e",
        "outputId": "0019d372-041b-4bde-a299-b1853dc0392c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/10] Sample 5640: Correctly Predicted: fast\n",
            "\n",
            "Collected 1 unique correct predicted words after 1000 attempts.\n",
            "\n",
            "Summary of correct predictions:\n",
            "    Sample Index Predicted Word Ground Truth Word\n",
            "0          5640           fast              fast\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "correct_predicted_results = []  # To store correct predictions only\n",
        "unique_correct_words = set()\n",
        "correct_predictions = 0\n",
        "attempts = 0\n",
        "max_attempts = 1000  # Safety cap\n",
        "\n",
        "# === Step 7: Predict from test set until 10 unique correctly predicted words are found ===\n",
        "while len(unique_correct_words) < 10 and attempts < max_attempts:\n",
        "    idx = random.choice(range(len(X_test)))\n",
        "    file_path = X_test[idx][0]\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    sample_features = df.values  # shape (135, 176)\n",
        "    sample_tensor = torch.tensor(sample_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = sign_model(sample_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    predicted_word = class_to_word.get(predicted_class, \"unknown\")\n",
        "    ground_truth_class = y_test[idx]\n",
        "    ground_truth_word = class_to_word.get(ground_truth_class, \"unknown\")\n",
        "\n",
        "    is_correct = (predicted_class == ground_truth_class)\n",
        "\n",
        "    if is_correct and predicted_word != \"unknown\" and predicted_word not in unique_correct_words:\n",
        "        correct_predicted_results.append({\n",
        "            \"Sample Index\": idx,\n",
        "            \"Predicted Word\": predicted_word,\n",
        "            \"Ground Truth Word\": ground_truth_word\n",
        "        })\n",
        "        unique_correct_words.add(predicted_word)\n",
        "        print(f\"[{len(unique_correct_words)}/10] Sample {idx}: Correctly Predicted: {predicted_word}\")\n",
        "\n",
        "    attempts += 1\n",
        "\n",
        "# === Step 8: Summary Results ===\n",
        "print(f\"\\nCollected {len(unique_correct_words)} unique correct predicted words after {attempts} attempts.\")\n",
        "\n",
        "# Optionally display as DataFrame\n",
        "results_df = pd.DataFrame(correct_predicted_results)\n",
        "print(\"\\nSummary of correct predictions:\\n\", results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fb0536-16aa-4d2a-b612-b589514b24a9",
      "metadata": {
        "id": "d8fb0536-16aa-4d2a-b612-b589514b24a9"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbba993-5089-46b4-a824-1d22e08e85a6",
      "metadata": {
        "id": "acbba993-5089-46b4-a824-1d22e08e85a6",
        "outputId": "a803c619-075e-4daa-d4c2-f6d3051eb853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/10] Sample 56801: Correctly Predicted: fast\n",
            "\n",
            "Collected 1 unique correct predicted words after 1000 attempts.\n",
            "\n",
            "Summary of correct predictions:\n",
            "    Sample Index Predicted Word Ground Truth Word\n",
            "0         56801           fast              fast\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "correct_predicted_results = []  # To store correct predictions only\n",
        "unique_correct_words = set()\n",
        "correct_predictions = 0\n",
        "attempts = 0\n",
        "max_attempts = 1000  # Safety cap\n",
        "\n",
        "# === Step 7: Predict from test set until 10 unique correctly predicted words are found ===\n",
        "while len(unique_correct_words) < 10 and attempts < max_attempts:\n",
        "    idx = random.choice(range(len(X_train)))\n",
        "    file_path = X_train[idx][0]\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    sample_features = df.values  # shape (135, 176)\n",
        "    sample_tensor = torch.tensor(sample_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = sign_model(sample_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    predicted_word = class_to_word.get(predicted_class, \"unknown\")\n",
        "    ground_truth_class = y_train[idx]\n",
        "    ground_truth_word = class_to_word.get(ground_truth_class, \"unknown\")\n",
        "\n",
        "    is_correct = (predicted_class == ground_truth_class)\n",
        "\n",
        "    if is_correct and predicted_word != \"unknown\" and predicted_word not in unique_correct_words:\n",
        "        correct_predicted_results.append({\n",
        "            \"Sample Index\": idx,\n",
        "            \"Predicted Word\": predicted_word,\n",
        "            \"Ground Truth Word\": ground_truth_word\n",
        "        })\n",
        "        unique_correct_words.add(predicted_word)\n",
        "        print(f\"[{len(unique_correct_words)}/10] Sample {idx}: Correctly Predicted: {predicted_word}\")\n",
        "\n",
        "    attempts += 1\n",
        "\n",
        "# === Step 8: Summary Results ===\n",
        "print(f\"\\nCollected {len(unique_correct_words)} unique correct predicted words after {attempts} attempts.\")\n",
        "\n",
        "# Optionally display as DataFrame\n",
        "results_df = pd.DataFrame(correct_predicted_results)\n",
        "print(\"\\nSummary of correct predictions:\\n\", results_df)\n",
        "\n",
        "# Extract only the correctly predicted words\n",
        "correctly_predicted_words = [entry['Predicted Word'] for entry in correct_predicted_results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f128863",
      "metadata": {
        "id": "1f128863"
      },
      "source": [
        "## WER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c101e3d6",
      "metadata": {
        "id": "c101e3d6",
        "outputId": "e8393372-cc2e-4028-95a9-40ad8d2252a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/10] Sample 37934: Correctly Predicted: fast\n",
            "[2/10] Sample 35417: Correctly Predicted: cry\n",
            "\n",
            "Collected 2 unique correct predicted words after 1000 attempts.\n",
            "Word Error Rate (WER): 0.9960\n",
            "\n",
            "Summary of correct predictions:\n",
            "    Sample Index Predicted Word Ground Truth Word\n",
            "0         37934           fast              fast\n",
            "1         35417            cry               cry\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def compute_wer(references, hypotheses):\n",
        "    \"\"\"\n",
        "    Compute Word Error Rate (WER) for isolated sign language predictions.\n",
        "\n",
        "    Args:\n",
        "        references (list): List of ground-truth glosses (e.g., [\"APPLE\", \"HOUSE\"]).\n",
        "        hypotheses (list): List of predicted glosses (e.g., [\"APPLE\", \"TREE\"]).\n",
        "\n",
        "    Returns:\n",
        "        float: WER as (substitutions + deletions + insertions) / total reference words.\n",
        "    \"\"\"\n",
        "    if not references:\n",
        "        return 0.0 if not hypotheses else float('inf')\n",
        "\n",
        "    substitutions = 0\n",
        "    deletions = 0\n",
        "    insertions = 0\n",
        "    total_words = len(references)\n",
        "\n",
        "    # For isolated signs, each pair is a single gloss\n",
        "    for ref, hyp in zip(references, hypotheses):\n",
        "        if ref != hyp:\n",
        "            substitutions += 1  # Incorrect prediction = substitution\n",
        "\n",
        "    # Handle length mismatches\n",
        "    if len(hypotheses) < len(references):\n",
        "        deletions += len(references) - len(hypotheses)\n",
        "    elif len(hypotheses) > len(references):\n",
        "        insertions += len(hypotheses) - len(references)\n",
        "\n",
        "    total_errors = substitutions + deletions + insertions\n",
        "    return total_errors / total_words if total_words > 0 else 0.0\n",
        "\n",
        "# Initialize lists for tracking predictions\n",
        "correct_predicted_results = []  # Store correct predictions\n",
        "unique_correct_words = set()    # Track unique correct glosses\n",
        "all_predictions = []            # Store all predictions for WER\n",
        "all_references = []            # Store all ground-truth glosses for WER\n",
        "attempts = 0\n",
        "max_attempts = 1000\n",
        "\n",
        "# === Step 7: Predict until 10 unique correctly predicted words are found ===\n",
        "while len(unique_correct_words) < 10 and attempts < max_attempts:\n",
        "    idx = random.choice(range(len(X_train)))\n",
        "    file_path = X_train[idx][0]\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    sample_features = df.values  # shape (135, 176)\n",
        "    sample_tensor = torch.tensor(sample_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = sign_model(sample_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    predicted_word = class_to_word.get(predicted_class, \"unknown\")\n",
        "    ground_truth_class = y_train[idx]\n",
        "    ground_truth_word = class_to_word.get(ground_truth_class, \"unknown\")\n",
        "\n",
        "    # Store predictions and references for WER\n",
        "    all_predictions.append(predicted_word)\n",
        "    all_references.append(ground_truth_word)\n",
        "\n",
        "    is_correct = (predicted_class == ground_truth_class)\n",
        "\n",
        "    if is_correct and predicted_word != \"unknown\" and predicted_word not in unique_correct_words:\n",
        "        correct_predicted_results.append({\n",
        "            \"Sample Index\": idx,\n",
        "            \"Predicted Word\": predicted_word,\n",
        "            \"Ground Truth Word\": ground_truth_word\n",
        "        })\n",
        "        unique_correct_words.add(predicted_word)\n",
        "        print(f\"[{len(unique_correct_words)}/10] Sample {idx}: Correctly Predicted: {predicted_word}\")\n",
        "\n",
        "    attempts += 1\n",
        "\n",
        "# === Step 8: Compute WER and Summarize Results ===\n",
        "wer = compute_wer(all_references, all_predictions)\n",
        "print(f\"\\nCollected {len(unique_correct_words)} unique correct predicted words after {attempts} attempts.\")\n",
        "print(f\"Word Error Rate (WER): {wer:.4f}\")\n",
        "\n",
        "# Display results as DataFrame\n",
        "results_df = pd.DataFrame(correct_predicted_results)\n",
        "print(\"\\nSummary of correct predictions:\\n\", results_df)\n",
        "\n",
        "# Extract correctly predicted words\n",
        "correctly_predicted_words = [entry['Predicted Word'] for entry in correct_predicted_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4898bd0-f043-4ad3-8a2a-351f457c7769",
      "metadata": {
        "id": "e4898bd0-f043-4ad3-8a2a-351f457c7769"
      },
      "source": [
        "## Sentence Generation with predicted words from signer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb08ca6a-68a2-4bc8-ba80-132894ad06a1",
      "metadata": {
        "id": "fb08ca6a-68a2-4bc8-ba80-132894ad06a1",
        "outputId": "51d195aa-780a-4ebe-e6af-94c9bb3b6702"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fast', 'milk', 'empty', 'aunt']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca2c932-aca4-4c29-8fae-537a6a9dc05f",
      "metadata": {
        "id": "4ca2c932-aca4-4c29-8fae-537a6a9dc05f"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "client= openai.OpenAI(api_key= \"API key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7cb8ea-eef4-417a-ab6f-a8c8bca39148",
      "metadata": {
        "id": "bf7cb8ea-eef4-417a-ab6f-a8c8bca39148"
      },
      "source": [
        "## Case 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9849cd20-c493-4fe9-b4c3-8b83888bb022",
      "metadata": {
        "id": "9849cd20-c493-4fe9-b4c3-8b83888bb022"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_with_gpt(predicted_words):\n",
        "    prompt = (\n",
        "        f\"Create a short, creative, grammatically correct sentence \"\n",
        "        f\"using ONLY these words: {', '.join(predicted_words)}. \"\n",
        "        f\"Do not add any extra words. You can rearrange them, repeat if needed, \"\n",
        "        f\"but do not introduce new words.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative sentence generator.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=20,\n",
        "        n=1,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59284ee-4bde-4286-a1a3-b829c68aad69",
      "metadata": {
        "id": "c59284ee-4bde-4286-a1a3-b829c68aad69",
        "outputId": "fb90314b-4794-4089-f6fe-e2628cbecc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: Aunt fast emptied the milk.\n"
          ]
        }
      ],
      "source": [
        "#predicted_words = ['kitty', 'milk', 'if', 'fast', 'finger']\n",
        "generated_sentence = generate_sentence_with_gpt(predicted_words)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e9922e-ad2d-494c-a08e-536b41b93643",
      "metadata": {
        "id": "f4e9922e-ad2d-494c-a08e-536b41b93643"
      },
      "source": [
        "## Case 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb1cd8b-a1be-4bef-ba69-0c85c01a0aa0",
      "metadata": {
        "id": "9eb1cd8b-a1be-4bef-ba69-0c85c01a0aa0",
        "outputId": "50b3ee72-d902-4e21-adf7-9b9b07833bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sign Language Sentence: Aunt fast empty milk.\n"
          ]
        }
      ],
      "source": [
        "def generate_sign_language_sentence(predicted_words):\n",
        "    prompt = (\n",
        "        f\"You are creating a meaningful, natural-sounding sentence suitable for a sign language performance. \"\n",
        "        f\"Use ONLY these words: {', '.join(predicted_words)}. \"\n",
        "        f\"Make the sentence visual, imaginative, and easy to express with body language. \"\n",
        "        f\"Do NOT introduce new words, but you can rearrange or repeat words creatively if needed. \"\n",
        "        f\"Keep it short and vivid.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a visual sentence crafter for sign language.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.2,  # more creativity\n",
        "        max_tokens=20,     # allow longer sentences\n",
        "        n=1,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Usage\n",
        "generated_sentence = generate_sign_language_sentence(predicted_words)\n",
        "print(\"\\nGenerated Sign Language Sentence:\", generated_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdde7f1-3ae3-4e77-b510-53ff34e8e6a6",
      "metadata": {
        "id": "9fdde7f1-3ae3-4e77-b510-53ff34e8e6a6",
        "outputId": "dd0b15df-0814-4d2c-c50c-f0200a9d2e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: Aunt fast empty milk.\n"
          ]
        }
      ],
      "source": [
        "generated_sentence1 = generate_sign_language_sentence(predicted_words)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe3b412-2577-41da-bdb3-950b470e0eef",
      "metadata": {
        "id": "dfe3b412-2577-41da-bdb3-950b470e0eef"
      },
      "source": [
        "## Case 1 : Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844ee22f",
      "metadata": {
        "id": "844ee22f"
      },
      "outputs": [],
      "source": [
        "# Rearranging manually to desired ASL order\n",
        "asl_ordered_words = ['Aunt', 'fast','empty','milk']\n",
        "asl_gloss_sentence = \" \".join(asl_ordered_words)\n",
        "reference_sentence=asl_gloss_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc5bb60",
      "metadata": {
        "id": "9fc5bb60",
        "outputId": "5f0aceac-403b-4d4b-9771-aac147e729ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Aunt fast empty milk'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "asl_gloss_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916c74d8-6d9b-4f72-b731-63130decff03",
      "metadata": {
        "id": "916c74d8-6d9b-4f72-b731-63130decff03",
        "outputId": "3e4e3cb3-929a-45fa-eeec-c293c8ee53f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: Fast Aunt emptied the milk.\n",
            "BLEU Score: 0.1269\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu_score(reference_sentence, generated_sentence):\n",
        "    \"\"\"\n",
        "    Compute BLEU score between reference and generated sentence.\n",
        "    \"\"\"\n",
        "    reference_tokens = reference_sentence.lower().split()\n",
        "    candidate_tokens = generated_sentence.lower().split()\n",
        "\n",
        "    smoothie = SmoothingFunction().method4  # smoothing helps with short sentences\n",
        "\n",
        "    score = sentence_bleu(\n",
        "        [reference_tokens],\n",
        "        candidate_tokens,\n",
        "        smoothing_function=smoothie,\n",
        "        weights=(0.5, 0.5)  # consider unigrams and bigrams equally\n",
        "    )\n",
        "    return score\n",
        "\n",
        "# Example Usage:\n",
        "#reference_sentence = \"The owl cut the puzzle pieces, alligator did the same.\"\n",
        "generated_sentence = generate_sentence_with_gpt(predicted_words)\n",
        "\n",
        "bleu = compute_bleu_score(reference_sentence, generated_sentence)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence)\n",
        "print(f\"BLEU Score: {bleu:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcdc63eb-2ff1-4825-af1e-8bcf9d996fe3",
      "metadata": {
        "id": "dcdc63eb-2ff1-4825-af1e-8bcf9d996fe3"
      },
      "source": [
        "## Case 2 : Bleu Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773f0988-e0bc-479d-b53e-53efa64b0dc8",
      "metadata": {
        "id": "773f0988-e0bc-479d-b53e-53efa64b0dc8",
        "outputId": "eb1fa59d-956d-4225-c0cf-1841ddde6740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: Aunt fast empty milk.\n",
            "BLEU Score: 0.7071\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu_score(reference_sentence, generated_sentence):\n",
        "    \"\"\"\n",
        "    Compute BLEU score between reference and generated sentence.\n",
        "    \"\"\"\n",
        "    reference_tokens = reference_sentence.lower().split()\n",
        "    candidate_tokens = generated_sentence.lower().split()\n",
        "\n",
        "    smoothie = SmoothingFunction().method4  # smoothing helps with short sentences\n",
        "\n",
        "    score = sentence_bleu(\n",
        "        [reference_tokens],\n",
        "        candidate_tokens,\n",
        "        smoothing_function=smoothie,\n",
        "        weights=(0.5, 0.5)  # consider unigrams and bigrams equally\n",
        "    )\n",
        "    return score\n",
        "\n",
        "# Example Usage:\n",
        "#reference_sentence = \"The owl cut the puzzle pieces, alligator did the same.\"\n",
        "generated_sentence = generate_sign_language_sentence(predicted_words)\n",
        "\n",
        "bleu = compute_bleu_score(reference_sentence, generated_sentence)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence)\n",
        "print(f\"BLEU Score: {bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16a65e7-ec13-4e7b-90a5-6016b62ebbe2",
      "metadata": {
        "id": "b16a65e7-ec13-4e7b-90a5-6016b62ebbe2"
      },
      "source": [
        "## Sentence Formation with correctly Predicted words /GLOSS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a233b414-de0a-4b73-900e-8a38029b42b3",
      "metadata": {
        "id": "a233b414-de0a-4b73-900e-8a38029b42b3",
        "outputId": "0194af7e-9ade-46db-9ae3-975d5bce3988"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cut', 'alligator', 'dog']"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correctly_predicted_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fa8dd5-57d4-4aa7-b3fd-20def568cc5d",
      "metadata": {
        "id": "05fa8dd5-57d4-4aa7-b3fd-20def568cc5d"
      },
      "outputs": [],
      "source": [
        "# Rearranging manually to desired ASL order\n",
        "asl_ordered_words = ['alligator', 'cut', 'dog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f176441-0ca1-46e6-a5f9-88038a2a9a30",
      "metadata": {
        "id": "9f176441-0ca1-46e6-a5f9-88038a2a9a30"
      },
      "outputs": [],
      "source": [
        "asl_gloss_sentence = \" \".join(asl_ordered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38e93aa-7e49-4788-8070-1e546df75ecf",
      "metadata": {
        "id": "d38e93aa-7e49-4788-8070-1e546df75ecf",
        "outputId": "4205a580-cfa0-4c4d-ac9a-609b589cafb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rearranged ASL Gloss Words: ['alligator', 'cut', 'dog']\n",
            "ASL Gloss Sentence: alligator cut dog\n"
          ]
        }
      ],
      "source": [
        "print(\"Rearranged ASL Gloss Words:\", asl_ordered_words)\n",
        "print(\"ASL Gloss Sentence:\", asl_gloss_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaa6640-f0bf-4f59-9775-2d6b7ff26ee4",
      "metadata": {
        "id": "dcaa6640-f0bf-4f59-9775-2d6b7ff26ee4",
        "outputId": "12ca99e7-ab67-4f35-ecd1-c9c76597c1af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: The dog cut the alligator.\n"
          ]
        }
      ],
      "source": [
        "generated_sentence1 = generate_sentence_with_gpt( asl_ordered_words)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85cf4691-f879-41a5-afcc-374048f42555",
      "metadata": {
        "id": "85cf4691-f879-41a5-afcc-374048f42555",
        "outputId": "785b99ed-4c35-433f-9d92-5fee155ba41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: The dog cut the alligator.\n"
          ]
        }
      ],
      "source": [
        "generated_sentence2 = generate_sign_language_sentence( asl_ordered_words)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c95a067-a2ed-4796-8f98-c4489cf862ab",
      "metadata": {
        "id": "5c95a067-a2ed-4796-8f98-c4489cf862ab",
        "outputId": "25b35d6c-89f5-4286-9acd-c755d83d585c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Sentence: The dog cut the alligator.\n",
            "BLEU Score: 0.1269\n"
          ]
        }
      ],
      "source": [
        "reference_sentence = asl_gloss_sentence\n",
        "\n",
        "\n",
        "bleu = compute_bleu_score(reference_sentence, generated_sentence1)\n",
        "print(\"\\nGenerated Sentence:\", generated_sentence1)\n",
        "print(f\"BLEU Score: {bleu:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e7d84d-4a8a-4f49-8685-b1a4ab3d749f",
      "metadata": {
        "id": "a5e7d84d-4a8a-4f49-8685-b1a4ab3d749f"
      },
      "outputs": [],
      "source": [
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0494df2-af05-40e5-8bab-1b8ef976805f",
      "metadata": {
        "id": "b0494df2-af05-40e5-8bab-1b8ef976805f",
        "outputId": "a3c07795-6fed-48cc-bb50-e28a43ff6a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: alligator cut dog\n",
            "Ground Truth: alligator cut dog\n",
            "WER: 1.3333\n"
          ]
        }
      ],
      "source": [
        "predicted_words = asl_ordered_words\n",
        "ground_truth_words =  asl_ordered_words\n",
        "\n",
        "# Convert to space-separated sentences\n",
        "#predicted_sentence = \" \".join(predicted_words)\n",
        "ground_truth_sentence = \" \".join(ground_truth_words)\n",
        "\n",
        "# WER calculation\n",
        "error_rate = wer(ground_truth_sentence, generated_sentence1)\n",
        "\n",
        "print(f\"Predicted: {generated_sentence1}\")\n",
        "print(f\"Ground Truth: {ground_truth_sentence}\")\n",
        "print(f\"WER: {error_rate:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25178e6e-4a0e-4eb6-b973-0c1a13aef341",
      "metadata": {
        "id": "25178e6e-4a0e-4eb6-b973-0c1a13aef341"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "asl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}