{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "120d7a0a-b5fa-4d83-957e-0609e0f82848",
      "metadata": {
        "id": "120d7a0a-b5fa-4d83-957e-0609e0f82848"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return (x + torch.abs(x)) / 2\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).float()\n",
        "\n",
        "def softmax(x):\n",
        "    x_stable = x - torch.max(x, dim=1, keepdim=True)[0]\n",
        "    exp_x = torch.exp(x_stable)\n",
        "    return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
        "\n",
        "def cross_entropy_loss(probs, labels):\n",
        "    N = probs.shape[0]\n",
        "    correct_probs = probs[torch.arange(N, device=probs.device), labels]\n",
        "    loss = -torch.log(correct_probs)\n",
        "    return loss.mean()\n",
        "\n",
        "def load_data(csv_path, device):\n",
        "    df = pd.read_csv(csv_path, skiprows=1, low_memory=False, header=None)\n",
        "    data = df.values\n",
        "    labels = data[:, 0].astype(int)\n",
        "    images = data[:, 1:].astype('float32') / 255.0  # Normalize to [0,1]\n",
        "    # Create tensors and then move them to the device\n",
        "    images = torch.tensor(images, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "pbem-x9rwcTC"
      },
      "id": "pbem-x9rwcTC",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czReDf263IxY",
        "outputId": "61353234-b64b-4887-d84c-9ff734c58329"
      },
      "id": "czReDf263IxY",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "in_features = 784\n",
        "hidden1 = 512\n",
        "hidden2 = 256\n",
        "hidden3 = 128\n",
        "out_features = 10\n",
        "\n",
        "# He initialization for each layer:\n",
        "W1 = torch.randn(in_features, hidden1, dtype=torch.float32, device=device) * math.sqrt(2.0/in_features)\n",
        "b1 = torch.zeros(1, hidden1, dtype=torch.float32, device=device)\n",
        "\n",
        "W2 = torch.randn(hidden1, hidden2, dtype=torch.float32, device=device) * math.sqrt(2.0/hidden1)\n",
        "b2 = torch.zeros(1, hidden2, dtype=torch.float32, device=device)\n",
        "\n",
        "W3 = torch.randn(hidden2, hidden3, dtype=torch.float32, device=device) * math.sqrt(2.0/hidden2)\n",
        "b3 = torch.zeros(1, hidden3, dtype=torch.float32, device=device)\n",
        "\n",
        "W4 = torch.randn(hidden3, out_features, dtype=torch.float32, device=device) * math.sqrt(2.0 / hidden3)\n",
        "b4 = torch.zeros(1, out_features, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "0yrQlusnRhv7"
      },
      "id": "0yrQlusnRhv7",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_images, train_labels = load_data('mnist_train.csv', device)\n",
        "test_images, test_labels   = load_data('mnist_test.csv', device)\n",
        "\n",
        "#Data to device:\n",
        "train_images = train_images.to(device)\n",
        "train_labels = train_labels.to(device)\n",
        "test_images = test_images.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "\n",
        "# Parameters\n",
        "lambda_l2 = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "num_train = train_images.shape[0]"
      ],
      "metadata": {
        "id": "uqUXoXXguqR-"
      },
      "id": "uqUXoXXguqR-",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam parameters\n",
        "lr = 0.0001\n",
        "beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "m, v = {}, {}\n",
        "params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4}\n",
        "for key in params:\n",
        "    m[key] = torch.zeros_like(params[key])\n",
        "    v[key] = torch.zeros_like(params[key])"
      ],
      "metadata": {
        "id": "NUQMUBtDzJwh"
      },
      "id": "NUQMUBtDzJwh",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    permutation = torch.randperm(num_train, device=device)\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "\n",
        "    for i in range(0, num_train, batch_size):\n",
        "        t += 1\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        X = train_images[indices]  # (B, 784)\n",
        "        y = train_labels[indices]  # (B,)\n",
        "        B = X.shape[0]\n",
        "\n",
        "        # Forward Pass\n",
        "        z1 = torch.matmul(X, W1) + b1         # (B, 512)\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = torch.matmul(a1, W2) + b2          # (B, 256)\n",
        "        a2 = relu(z2)\n",
        "\n",
        "        z3 = torch.matmul(a2, W3) + b3          # (B, 128)\n",
        "        a3 = relu(z3)\n",
        "\n",
        "        logits = torch.matmul(a3, W4) + b4      # (B, 10)\n",
        "        probs = softmax(logits)                # (B, 10)\n",
        "\n",
        "        loss = cross_entropy_loss(probs, y)\n",
        "        # Regularization loss added to cross-entropy loss:\n",
        "        reg_loss = lambda_l2 * (torch.sum(W1**2) + torch.sum(W2**2) + torch.sum(W3**2) + torch.sum(W4**2))\n",
        "        loss_total = loss + reg_loss\n",
        "\n",
        "        # Backward Pass (manual gradients)\n",
        "        one_hot = torch.zeros_like(probs)\n",
        "        one_hot[torch.arange(B, device=device), y] = 1\n",
        "\n",
        "        d_logits = (probs - one_hot) / B  # (B, 10)\n",
        "\n",
        "        dW4 = torch.matmul(a3.t(), d_logits)         # (hidden3, 10)\n",
        "        db4 = d_logits.sum(dim=0, keepdim=True)          # (1, 10)\n",
        "\n",
        "        d_a3 = torch.matmul(d_logits, W4.t())            # (B, hidden3)\n",
        "        d_z3 = d_a3 * relu_derivative(z3)                # (B, hidden3)\n",
        "\n",
        "        dW3 = torch.matmul(a2.t(), d_z3)                # (hidden2, hidden3)\n",
        "        db3 = d_z3.sum(dim=0, keepdim=True)              # (1, hidden3)\n",
        "\n",
        "        d_a2 = torch.matmul(d_z3, W3.t())                # (B, hidden2)\n",
        "        d_z2 = d_a2 * relu_derivative(z2)                # (B, hidden2)\n",
        "\n",
        "        dW2 = torch.matmul(a1.t(), d_z2)               # (hidden1, hidden2)\n",
        "        db2 = d_z2.sum(dim=0, keepdim=True)              # (1, hidden2)\n",
        "\n",
        "        d_a1 = torch.matmul(d_z2, W2.t())                # (B, hidden1)\n",
        "        d_z1 = d_a1 * relu_derivative(z1)                # (B, hidden1)\n",
        "\n",
        "        dW1 = torch.matmul(X.t(), d_z1)                # (in_features, hidden1)\n",
        "        db1 = d_z1.sum(dim=0, keepdim=True)              # (1, hidden1)\n",
        "\n",
        "        # Add regularization gradients: derivative of reg_loss is 2 * lambda_l2 * W\n",
        "        dW1 += 2 * lambda_l2 * W1\n",
        "        dW2 += 2 * lambda_l2 * W2\n",
        "        dW3 += 2 * lambda_l2 * W3\n",
        "        dW4 += 2 * lambda_l2 * W4\n",
        "\n",
        "        # Adam Optimizer update\n",
        "        # Collect gradients into a dictionary\n",
        "        grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3, 'W4': dW4, 'b4': db4}\n",
        "\n",
        "        # Initialize bias-corrected moment estimates\n",
        "        m_hat, v_hat = {}, {}\n",
        "\n",
        "        # Update moving averages of the gradients (m) and squared gradients (v)\n",
        "        for key in params:\n",
        "            m[key] = beta1 * m[key] + (1 - beta1) * grads[key]\n",
        "            v[key] = beta2 * v[key] + (1 - beta2) * grads[key]**2\n",
        "            m_hat[key] = m[key] / (1 - beta1**t) # Compute bias-corrected first moment estimate\n",
        "            v_hat[key] = v[key] / (1 - beta2**t) # Compute bias-corrected second moment estimate\n",
        "\n",
        "        # Update parameters using Adam optimizer\n",
        "        W1 -= lr * m_hat['W1'] / (torch.sqrt(v_hat['W1']) + epsilon)\n",
        "        b1 -= lr * m_hat['b1'] / (torch.sqrt(v_hat['b1']) + epsilon)\n",
        "        W2 -= lr * m_hat['W2'] / (torch.sqrt(v_hat['W2']) + epsilon)\n",
        "        b2 -= lr * m_hat['b2'] / (torch.sqrt(v_hat['b2']) + epsilon)\n",
        "        W3 -= lr * m_hat['W3'] / (torch.sqrt(v_hat['W3']) + epsilon)\n",
        "        b3 -= lr * m_hat['b3'] / (torch.sqrt(v_hat['b3']) + epsilon)\n",
        "        W4 -= lr * m_hat['W4'] / (torch.sqrt(v_hat['W4']) + epsilon)\n",
        "        b4 -= lr * m_hat['b4'] / (torch.sqrt(v_hat['b4']) + epsilon)\n",
        "\n",
        "        # Accumulate total loss and correct predictions for accuracy calculation\n",
        "        running_loss += loss_total.item() * B\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        correct_train += (preds == y).sum().item()\n",
        "\n",
        "    # Compute average training loss and accuracy\n",
        "    train_loss = running_loss / num_train\n",
        "    train_accuracy = 100.0 * correct_train / num_train\n",
        "\n",
        "    # Evaluate on Test Set\n",
        "    z1_test = torch.matmul(test_images, W1) + b1\n",
        "    a1_test = relu(z1_test)\n",
        "    z2_test = torch.matmul(a1_test, W2) + b2\n",
        "    a2_test = relu(z2_test)\n",
        "    z3_test = torch.matmul(a2_test, W3) + b3\n",
        "    a3_test = relu(z3_test)\n",
        "    logits_test = torch.matmul(a3_test, W4) + b4\n",
        "    probs_test = softmax(logits_test)\n",
        "\n",
        "    # Compute average test loss and accuracy\n",
        "    test_loss = cross_entropy_loss(probs_test, test_labels)\n",
        "    reg_loss_test = lambda_l2 * (torch.sum(W1**2) + torch.sum(W2**2) + torch.sum(W3**2) + torch.sum(W4**2))\n",
        "    test_loss_total = test_loss + reg_loss_test  # Final test loss with regularization\n",
        "\n",
        "    preds_test = torch.argmax(probs_test, dim=1)\n",
        "    test_accuracy = 100.0 * (preds_test == test_labels).sum().item() / test_labels.shape[0]\n",
        "\n",
        "    # Printing only epochs 5 and 10\n",
        "    #if epoch + 1 in [5, 10]:\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "          f\"Test Loss: {test_loss_total:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Store final metrics\n",
        "    final_train_loss = train_loss\n",
        "    final_train_accuracy = train_accuracy\n",
        "    final_test_loss = test_loss_total\n",
        "    final_test_accuracy = test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDrdKni2RuC2",
        "outputId": "4f462ac2-ad94-408d-c86d-61d65d746c99"
      },
      "id": "pDrdKni2RuC2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: Train Loss: 1.5778, Train Acc: 90.44%, Test Loss: 1.1280, Test Acc: 95.36%\n",
            "Epoch 2/10: Train Loss: 0.9868, Train Acc: 95.71%, Test Loss: 0.8551, Test Acc: 96.35%\n",
            "Epoch 3/10: Train Loss: 0.7611, Train Acc: 96.63%, Test Loss: 0.6758, Test Acc: 96.92%\n",
            "Epoch 4/10: Train Loss: 0.6087, Train Acc: 97.18%, Test Loss: 0.5539, Test Acc: 96.97%\n",
            "Epoch 5/10: Train Loss: 0.4984, Train Acc: 97.54%, Test Loss: 0.4650, Test Acc: 97.03%\n",
            "Epoch 6/10: Train Loss: 0.4194, Train Acc: 97.70%, Test Loss: 0.3965, Test Acc: 97.25%\n",
            "Epoch 7/10: Train Loss: 0.3590, Train Acc: 97.97%, Test Loss: 0.3479, Test Acc: 97.53%\n",
            "Epoch 8/10: Train Loss: 0.3153, Train Acc: 98.11%, Test Loss: 0.3090, Test Acc: 97.70%\n",
            "Epoch 9/10: Train Loss: 0.2826, Train Acc: 98.22%, Test Loss: 0.2920, Test Acc: 97.51%\n",
            "Epoch 10/10: Train Loss: 0.2582, Train Acc: 98.33%, Test Loss: 0.2742, Test Acc: 97.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Results after all epochs\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"Train Loss: {final_train_loss:.4f}, Train Acc: {final_train_accuracy:.2f}%\")\n",
        "print(f\"Test Loss: {final_test_loss:.4f}, Test Acc: {final_test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "6xVQz0RN4Bfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb78f756-c83f-4dc0-fcf4-64c7efa8e374"
      },
      "id": "6xVQz0RN4Bfu",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results:\n",
            "Train Loss: 0.2582, Train Acc: 98.33%\n",
            "Test Loss: 0.2742, Test Acc: 97.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define constants\n",
        "float_size = 4  # float32 = 4 bytes\n",
        "\n",
        "# Network layer shapes: [784, 512, 256, 128, 10]\n",
        "# Total parameters: weights + biases\n",
        "W1_params = 784 * 512  # 401,408\n",
        "b1_params = 512        # 512\n",
        "W2_params = 512 * 256  # 131,072\n",
        "b2_params = 256        # 256\n",
        "W3_params = 256 * 128  # 32,768\n",
        "b3_params = 128        # 128\n",
        "W4_params = 128 * 10   # 1,280\n",
        "b4_params = 10         # 10\n",
        "\n",
        "total_weights = W1_params + b1_params + W2_params + b2_params + W3_params + b3_params + W4_params + b4_params  # 567,434\n",
        "\n",
        "# Activations and pre-activations stored in forward pass per sample\n",
        "a0 = 784          # Input\n",
        "z1_a1 = 512 * 2   # Z1 + A1\n",
        "z2_a2 = 256 * 2   # Z2 + A2\n",
        "z3_a3 = 128 * 2   # Z3 + A3\n",
        "z4_a4 = 10 * 2    # Z4 + A4 (output)\n",
        "activations_per_sample = a0 + z1_a1 + z2_a2 + z3_a3 + z4_a4  # 2,596\n",
        "\n",
        "# Intermediate gradients in backward pass per sample (dZ1, dA1, dZ2, dA2, dZ3, dA3, dZ4)\n",
        "deltas_per_sample = 512 + 512 + 256 + 256 + 128 + 128 + 10  # 1,802\n",
        "adam_states = 2 * total_weights                              # 1,134,868 (m and v for each parameter)\n",
        "\n",
        "batch_sizes = [1, 32, 1024]\n",
        "results = []\n",
        "\n",
        "for B in batch_sizes:\n",
        "    # ----- Forward Pass Memory -----\n",
        "    # Parameters (weights + biases)\n",
        "    params_bytes = total_weights * float_size\n",
        "    params_mb = params_bytes / 1_000_000\n",
        "\n",
        "    # Activations (per batch)\n",
        "    activations_vals = activations_per_sample * B\n",
        "    activations_bytes = activations_vals * float_size\n",
        "    activations_mb = activations_bytes / 1_000_000\n",
        "\n",
        "    # Total Forward Pass\n",
        "    forward_bytes = params_bytes + activations_bytes\n",
        "    forward_mb = forward_bytes / 1_000_000\n",
        "\n",
        "    # ----- Backward Pass (SGD) Memory -----\n",
        "    backward_vals_sgd = total_weights + deltas_per_sample * B  # Parameters + intermediates\n",
        "    backward_bytes_sgd = backward_vals_sgd * float_size\n",
        "    backward_mb_sgd = backward_bytes_sgd / 1_000_000\n",
        "\n",
        "    # ----- Backward Pass (Adam) Memory -----\n",
        "    backward_bytes_adam = backward_bytes_sgd + adam_states * float_size\n",
        "    backward_mb_adam = backward_bytes_adam / 1_000_000\n",
        "\n",
        "    # ----- Totals -----\n",
        "    total_mb_sgd = forward_mb + backward_mb_sgd\n",
        "    total_mb_adam = forward_mb + backward_mb_adam\n",
        "\n",
        "    results.append({\n",
        "        \"Batch Size\": B,\n",
        "        \"Forward Pass Total (MB)\": round(forward_mb, 3),\n",
        "        \"Backward Pass SGD (MB)\": round(backward_mb_sgd, 3),\n",
        "        \"Backward Pass Adam (MB)\": round(backward_mb_adam, 3),\n",
        "        \"Total (SGD) MB\": round(total_mb_sgd, 3),\n",
        "        \"Total (Adam) MB\": round(total_mb_adam, 3)\n",
        "    })\n",
        "\n",
        "memory_summary_df = pd.DataFrame(results)\n",
        "print(memory_summary_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRMEm7nSp1tn",
        "outputId": "7aa48fd2-e1c6-4edb-d77a-81b62296a3d9"
      },
      "id": "GRMEm7nSp1tn",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Batch Size  Forward Pass Total (MB)  Backward Pass SGD (MB)  Backward Pass Adam (MB)  Total (SGD) MB  Total (Adam) MB\n",
            "          1                    2.280                   2.277                    6.816           4.557            9.097\n",
            "         32                    2.602                   2.500                    7.040           5.102            9.642\n",
            "       1024                   12.903                   9.651                   14.190          22.554           27.093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yF_u1_kzAXBD"
      },
      "id": "yF_u1_kzAXBD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}