{"cells":[{"cell_type":"markdown","metadata":{"id":"7TmenLyt2psf"},"source":["#### Installing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eizIELlZ2bTf","outputId":"9a89ee72-d609-456a-b829-7b674ae99a95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n","Requirement already satisfied: torchtext==0.15.1 in /usr/local/lib/python3.10/dist-packages (0.15.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.32.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.23.5)\n","Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (0.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.30.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n"]}],"source":["!pip install torch==2.0.0 torchtext==0.15.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9Pf1r0q23Dl","outputId":"d3bcb8f8-87c7-4232-976a-f3d468852b8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.32.3)\n","Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.45.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.30.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (18.1.8)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n"]}],"source":["!pip install torchdata==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yUwJg9F25Xy"},"outputs":[],"source":["!pip install portalocker>=2.0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OE4P-Jh527sh","outputId":"cef3c242-08e0-4911-c235-357be2bc74fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"]}],"source":["!pip install numpy==1.23.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vp3K9B5R29gC","outputId":"5ddbf16c-b6d8-49dc-8882-cc55d68c1d34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting de-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.23.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download de_core_news_sm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"markdown","metadata":{"id":"ML8miXo_3Bui"},"source":["#### Step2: Train a model on the training set (English-to-German)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82Fdwu2N2_Ls"},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import multi30k, Multi30k\n","from typing import Iterable, List\n","multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n","multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n","SOURCE_LANG = 'en'\n","TARGET_LANG = 'de'\n","tokenizer_map = {}\n","vocab_map = {}\n"]},{"cell_type":"markdown","metadata":{"id":"0G--Lh8n4lLN"},"source":["It contains, the get_tokenizer for tokenization, the build_vocab_from_iterator for vocabularies and datasets multi30k and Multi30k which are commonly adopted datasets for evaluation of English German translation models. The training and validation dataset URLs are specified to certain websites manually for the purpose of making sure that the sources are well utilized. As seen in the code, the SOURCE_LANG and TARGET_LANG variables are assigned values English (de) and  German (de) respectively. tokenizer_map and vocab_map dictionaries are also defined to store tokenizers and vocabularies specific to the language. The tokenizers will break down the unformatted text into tokens or words or subwords and the vocabularies will link these tokens to numbers which are inputs needed for the model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2sac78q4gBX"},"outputs":[],"source":["tokenizer_map[SOURCE_LANG] = get_tokenizer('spacy', language='en_core_web_sm')\n","tokenizer_map[TARGET_LANG] = get_tokenizer('spacy', language='de_core_news_sm')\n","def extract_tokens(data_iterator: Iterable, lang: str) -> List[str]:\n","    lang_index_map = {SOURCE_LANG: 0, TARGET_LANG: 1}\n","    for sample in data_iterator:\n","        yield tokenizer_map[lang](sample[lang_index_map[lang]])\n","UNKNOWN_IDX, PADDING_IDX, START_IDX, END_IDX = 0, 1, 2, 3\n","special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n","for lang in [SOURCE_LANG, TARGET_LANG]:\n","    train_data_iter = Multi30k(split='train', language_pair=(SOURCE_LANG, TARGET_LANG))\n","    vocab_map[lang] = build_vocab_from_iterator(extract_tokens(train_data_iter, lang),\n","                                                min_freq=1,\n","                                                specials=special_tokens,\n","                                                special_first=True)\n","for lang in [SOURCE_LANG, TARGET_LANG]:\n","    vocab_map[lang].set_default_index(UNKNOWN_IDX)"]},{"cell_type":"markdown","metadata":{"id":"yxmAZrlI4-V4"},"source":["The above code provides configuration to the tokenizers and vocabularies of the source English and target German languages in the given text data for a machine translation model implemented in the Pytorch's TorchText library. It first loads the English and German language models from spaCy library, which is among the most widely used NLP libraries, in order to initialize the tokenizers for both languages. Thereafter, the extract_tokens function walks through the dataset and makes use of the suitable tokenizer depending on the language. The first language is indexed at 0 and second at 1 with text samples being obtained at each of the two data iterator. Unknown words, padding, the start of a sequence and the end of a sequence are all represented by special tokens. The build_vocab_from_iterator function creates vocabularies of the two languages through the tokenized training and allows these special tokens on top of the created vocabularies. Moreover, it also changes each vocabulary to recognize default unknown word index."]},{"cell_type":"markdown","metadata":{"id":"At0eajWu5Iyu"},"source":["#### Seq2Seq Network using Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dn2-fhs1441B"},"outputs":[],"source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 embedding_dim: int,\n","                 dropout_rate: float,\n","                 max_sequence_length: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        denominator = torch.exp(- torch.arange(0, embedding_dim, 2) * math.log(10000) / embedding_dim)\n","        position = torch.arange(0, max_sequence_length).reshape(max_sequence_length, 1)\n","        position_embedding = torch.zeros((max_sequence_length, embedding_dim))\n","        position_embedding[:, 0::2] = torch.sin(position * denominator)\n","        position_embedding[:, 1::2] = torch.cos(position * denominator)\n","        position_embedding = position_embedding.unsqueeze(-2)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.register_buffer('position_embedding', position_embedding)\n","    def forward(self, token_embeddings: Tensor):\n","        return self.dropout(token_embeddings + self.position_embedding[:token_embeddings.size(0), :])\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, embedding_dim: int):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding_dim = embedding_dim\n","    def forward(self, tokens: Tensor):\n","        return self.embedding_layer(tokens.long()) * math.sqrt(self.embedding_dim)\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 encoder_layers: int,\n","                 decoder_layers: int,\n","                 embedding_dim: int,\n","                 num_heads: int,\n","                 source_vocab_size: int,\n","                 target_vocab_size: int,\n","                 feedforward_dim: int = 512,\n","                 dropout_rate: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer_model = Transformer(d_model=embedding_dim,\n","                                             nhead=num_heads,\n","                                             num_encoder_layers=encoder_layers,\n","                                             num_decoder_layers=decoder_layers,\n","                                             dim_feedforward=feedforward_dim,\n","                                             dropout=dropout_rate)\n","        self.output_layer = nn.Linear(embedding_dim, target_vocab_size)\n","        self.source_token_embedding = TokenEmbedding(source_vocab_size, embedding_dim)\n","        self.target_token_embedding = TokenEmbedding(target_vocab_size, embedding_dim)\n","        self.position_encoding = PositionalEncoding(embedding_dim, dropout_rate)\n","    def forward(self,\n","                source: Tensor,\n","                target: Tensor,\n","                source_mask: Tensor,\n","                target_mask: Tensor,\n","                source_padding_mask: Tensor,\n","                target_padding_mask: Tensor,\n","                memory_padding_mask: Tensor):\n","        source_embedding = self.position_encoding(self.source_token_embedding(source))\n","        target_embedding = self.position_encoding(self.target_token_embedding(target))\n","        output = self.transformer_model(source_embedding, target_embedding, source_mask, target_mask, None,\n","                                        source_padding_mask, target_padding_mask, memory_padding_mask)\n","        return self.output_layer(output)\n","    def encode(self, source: Tensor, source_mask: Tensor):\n","        return self.transformer_model.encoder(self.position_encoding(self.source_token_embedding(source)), source_mask)\n","    def decode(self, target: Tensor, memory: Tensor, target_mask: Tensor):\n","        return self.transformer_model.decoder(self.position_encoding(self.target_token_embedding(target)), memory,\n","                                              target_mask)"]},{"cell_type":"markdown","metadata":{"id":"x44vZFLL5QdB"},"source":["The above code provides the details of a seq2seq architecture aimed at machine translation through the transformer model using the Pytorch framework. The PositionalEncoding class appends positional numbers to token embeddings for a better understanding of the sequences since transformers do not make use of the sequential context. It generates position embeddings through sine and cosine functions and implements dropout for regularization purposes. Dense vector representations are provided to input tokens through the TokenEmbedding class which normalizes the embeddings by taking the square root of the embedding dimension . The primary class Seq2SeqTransformer unifies the state of the art transformer based model constituents: it creates a transformer architecture which has the required encoder and decoder layers, multi-head attention heads, and feed-forward layers together with their dimensions. It further has linear layers at the output to enable the representation of the hidden states to the size of vocabulary of the target language. The forward method executes the entire encoding and decoding process sequentially in an integrated fashion, embedding sources and targets and passing them through the transformer model. The encode method wraps and sequences the source input while the decode method targets the encoded source memory for the target input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SGRpI5s5LL7"},"outputs":[],"source":["def generate_upper_triangle_mask(size):\n","    mask = (torch.triu(torch.ones((size, size), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","def create_masks(source, target):\n","    source_sequence_length = source.shape[0]\n","    target_sequence_length = target.shape[0]\n","    target_mask = generate_upper_triangle_mask(target_sequence_length)\n","    source_mask = torch.zeros((source_sequence_length, source_sequence_length), device=DEVICE).type(torch.bool)\n","    source_padding_mask = (source == PADDING_IDX).transpose(0, 1)\n","    target_padding_mask = (target == PADDING_IDX).transpose(0, 1)\n","    return source_mask, target_mask, source_padding_mask, target_padding_mask"]},{"cell_type":"markdown","metadata":{"id":"FBud8GaA5Wo2"},"source":["The above code allows us to define two auxiliary functions: generate_upper_triangle_mask and create_masks that are useful in the implementation of attention mechanisms in the Transformer model. The generate_upper_triangle_mask function performs the opposite process, that of creating a mask for the target sequence used in the decoding. It creates a matrix containing 1’s above the diagonal using torch.triu, which indicates to which tokens can be paid attention to by the model. This will make it so that the model doesn’t look at future tokens, an autoregressive property during training is thus achieved. Hence negative infinity (-inf) is used to mask areas that aren’t allowed while the zeros are used to mask the areas that are allowed ensuring only current and past tokens are used.\n","\n","The third function under consideration is the create_masks function. It takes care of all the missing masks that are to be used for both the source as well as the target sequences. It first assesses the lengths of the source and target input sequences. It employs the upper triangle mask function in order to create the upper part of target_mask while setting all values of source_mask to 0 (matrix) which implicitly allows attention for the source input. The function also creates source_padding_mask and target_padding_mask, which are padding masks for the source and target sequences, respectively. These masks are used for padding tokens (PADDING_IDX) in the input sequences and prevent them from being attended during the attention mechanism."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BOD_sgi5SeP"},"outputs":[],"source":["torch.manual_seed(0)\n","SOURCE_VOCAB_SIZE = len(vocab_map[SOURCE_LANG])\n","TARGET_VOCAB_SIZE = len(vocab_map[TARGET_LANG])\n","EMBEDDING_DIM = 512\n","NUM_HEADS = 8\n","FEEDFORWARD_DIM = 512\n","BATCH_SIZE = 128\n","ENCODER_LAYERS = 3\n","DECODER_LAYERS = 3\n","seq2seq_transformer = Seq2SeqTransformer(ENCODER_LAYERS, DECODER_LAYERS, EMBEDDING_DIM,\n","                                         NUM_HEADS, SOURCE_VOCAB_SIZE, TARGET_VOCAB_SIZE, FEEDFORWARD_DIM)\n","for param in seq2seq_transformer.parameters():\n","    if param.dim() > 1:\n","        nn.init.xavier_uniform_(param)\n","seq2seq_transformer = seq2seq_transformer.to(DEVICE)\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=PADDING_IDX)\n","optimizer = torch.optim.Adam(seq2seq_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"]},{"cell_type":"markdown","metadata":{"id":"9d_dw3dG5ii8"},"source":["First, in the code, a random state seed is set for the purposes of repeating the experiments. From the vocabularies that have been built earlier (vocab_map), the sizes of the source and target vocabularies are drawn. In particular, values such as EMBEDDING_DIM, NUM_HEADS, FEEDFORWARD_DIM, BATCH_SIZE, ENCODER_LAYERS, DECODER_LAYERS are set. The Seq2SeqTransformer model is created in accordance with these parameters, including the number of encoder and decoder layers, the embedding dimension, the number of attention heads, and the size of the feedforward network.\n","\n","Moreover, the model accounts for Xavier uniform initialization at the beginning which sidetracks the issues of instability during training by ensuring that weights are not too big or too small. Then the model is put on the relevant device, either CPU or GPU. The loss function is CrossEntropyLoss with default ignore_index for padding tokens, in this case, the loss should not be affected by the padding. Adam optimizer is used with a learning rate of 0.0001, betas for momentum of 0.9 and 0.98; and smaller epsilon value of 1e-9, to prevent division by zero."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7PtX65o5h2A"},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","def apply_transforms_in_sequence(*transforms):\n","    def inner_function(text_input):\n","        for transform in transforms:\n","            text_input = transform(text_input)\n","        return text_input\n","    return inner_function\n","def convert_to_tensor(token_ids: List[int]):\n","    return torch.cat((torch.tensor([START_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([END_IDX])))\n","text_transform_map = {}\n","for lang in [SOURCE_LANG, TARGET_LANG]:\n","    text_transform_map[lang] = apply_transforms_in_sequence(tokenizer_map[lang],\n","                                                            vocab_map[lang],\n","                                                            convert_to_tensor)\n","def collate_batch_fn(batch):\n","    source_batch, target_batch = [], []\n","    for source_sample, target_sample in batch:\n","        source_batch.append(text_transform_map[SOURCE_LANG](source_sample.rstrip(\"\\n\")))\n","        target_batch.append(text_transform_map[TARGET_LANG](target_sample.rstrip(\"\\n\")))\n","\n","    source_batch = pad_sequence(source_batch, padding_value=PADDING_IDX)\n","    target_batch = pad_sequence(target_batch, padding_value=PADDING_IDX)\n","    return source_batch, target_batch"]},{"cell_type":"markdown","metadata":{"id":"md4LD5Wh5seG"},"source":["It comprises apply_transforms_in_sequence method that takes a list of text transforming steps and performs them in a sequence, which includes tokenization and vocabulary mapping. The convert_to_tensor function glosses the token IDs with starting token (START_IDX) and ending token (END_IDX). The transformation map into text_transform_map is a dictionary that contains both the source German language and the English target language transformation pipeline. The collate_batch_fn function aligns features from a sampled batch of text samples through transformations. These features include transforming of target and source raw text to text sequences of numbers. Subsequently, these sequences have been padded using pad_sequence and a padding token (PADDING_IDX) added to it in order to make all the sequences have the same length so as to be ready for training during model input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIljoVU65oTv"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","def train_one_epoch(seq2seq_transformer, optimizer):\n","    seq2seq_transformer.train()\n","    total_loss = 0\n","    training_data_iter = Multi30k(split='train', language_pair=(SOURCE_LANG, TARGET_LANG))\n","    training_dataloader = DataLoader(training_data_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch_fn)\n","    for source_batch, target_batch in training_dataloader:\n","        source_batch = source_batch.to(DEVICE)\n","        target_batch = target_batch.to(DEVICE)\n","        target_input = target_batch[:-1, :]\n","        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source_batch, target_input)\n","        logits = seq2seq_transformer(source_batch, target_input, source_mask, target_mask,\n","                                     source_padding_mask, target_padding_mask, source_padding_mask)\n","        optimizer.zero_grad()\n","        target_output = target_batch[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(list(training_dataloader))\n","def evaluate_model(seq2seq_transformer):\n","    seq2seq_transformer.eval()\n","    total_loss = 0\n","    validation_data_iter = Multi30k(split='valid', language_pair=(SOURCE_LANG, TARGET_LANG))\n","    validation_dataloader = DataLoader(validation_data_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch_fn)\n","    for source_batch, target_batch in validation_dataloader:\n","        source_batch = source_batch.to(DEVICE)\n","        target_batch = target_batch.to(DEVICE)\n","        target_input = target_batch[:-1, :]\n","        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source_batch, target_input)\n","        logits = seq2seq_transformer(source_batch, target_input, source_mask, target_mask,\n","                                     source_padding_mask, target_padding_mask, source_padding_mask)\n","        target_output = target_batch[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n","        total_loss += loss.item()\n","    return total_loss / len(list(validation_dataloader))"]},{"cell_type":"markdown","metadata":{"id":"tFEFxF0N50vj"},"source":["The above code presents the training and evaluation loops of the sequence to sequence (Seq2Seq) Transformer model within the context of a machine translation task, based on PyTorch framework. The train_one_epoch method conducts the training for one epoch by training on the training set from the Multi30k dataset, which is a benchmark dataset for German-English translation. To this end, it leverages a DataLoader as a means for batching and utilizes the collate_batch_fn function for padding purposes. For each training step, the source and target sequences are then transferred to the target device. The target sequence includes two parts target_input and target_output where the last token is cut from the input and the first token is cut from the output. The attention mechanisms of the model are preceded by sources of information (source_mask, target_mask) and padding masks, generated in advance by the create_masks function. In this way, the logits output by the embeddings are obtained through the Transformer architecture. The computation of cross-entropy loss is straightforward and involves the usage of CrossEntropyLoss while loss.backward() is used to compute gradients of loss. The parameters of the model to minimize the loss is updated by the optimizer and the last for the epoch is the average loss of that epoch.\n","\n","The evaluate_model function executes a comparable procedure as above but only for the validation dataset. In this case, the model's weights are not updated. The function operates in a loop processing the validation set, preparing the masks, forwarding the data, and calculating the loss for each batch. Also, since eval() is executed, there's no dropout, which simplifies the inference. The output of the function is the mean loss over the validation sample."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBNkQH8Q5yC7","outputId":"699e6bbc-0fab-43be-e2c2-c3c82a947833"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Training Loss: 5.900, Validation Loss: 4.628, Epoch Time = 52.793s\n","Epoch: 2, Training Loss: 4.125, Validation Loss: 3.782, Epoch Time = 51.241s\n","Epoch: 3, Training Loss: 3.459, Validation Loss: 3.313, Epoch Time = 51.339s\n","Epoch: 4, Training Loss: 3.021, Validation Loss: 3.010, Epoch Time = 51.100s\n","Epoch: 5, Training Loss: 2.697, Validation Loss: 2.811, Epoch Time = 51.124s\n","Epoch: 6, Training Loss: 2.440, Validation Loss: 2.645, Epoch Time = 51.360s\n","Epoch: 7, Training Loss: 2.228, Validation Loss: 2.505, Epoch Time = 52.057s\n","Epoch: 8, Training Loss: 2.051, Validation Loss: 2.380, Epoch Time = 51.788s\n","Epoch: 9, Training Loss: 1.901, Validation Loss: 2.298, Epoch Time = 51.334s\n","Epoch: 10, Training Loss: 1.772, Validation Loss: 2.231, Epoch Time = 51.177s\n","Epoch: 11, Training Loss: 1.653, Validation Loss: 2.206, Epoch Time = 51.003s\n","Epoch: 12, Training Loss: 1.545, Validation Loss: 2.166, Epoch Time = 51.259s\n","Epoch: 13, Training Loss: 1.448, Validation Loss: 2.157, Epoch Time = 51.189s\n","Epoch: 14, Training Loss: 1.361, Validation Loss: 2.114, Epoch Time = 52.173s\n","Epoch: 15, Training Loss: 1.280, Validation Loss: 2.072, Epoch Time = 52.416s\n","Epoch: 16, Training Loss: 1.207, Validation Loss: 2.053, Epoch Time = 51.501s\n","Epoch: 17, Training Loss: 1.139, Validation Loss: 2.039, Epoch Time = 51.102s\n","Epoch: 18, Training Loss: 1.070, Validation Loss: 2.019, Epoch Time = 50.998s\n","Epoch: 19, Training Loss: 1.009, Validation Loss: 2.030, Epoch Time = 51.021s\n","Epoch: 20, Training Loss: 0.952, Validation Loss: 2.032, Epoch Time = 51.117s\n"]}],"source":["from timeit import default_timer as timer\n","EPOCHS = 20\n","for epoch in range(1, EPOCHS + 1):\n","    start_time = timer()\n","    training_loss = train_one_epoch(seq2seq_transformer, optimizer)\n","    end_time = timer()\n","    validation_loss = evaluate_model(seq2seq_transformer)\n","    print(f\"Epoch: {epoch}, Training Loss: {training_loss:.3f}, Validation Loss: {validation_loss:.3f}, Epoch Time = {(end_time - start_time):.3f}s\")\n","def greedy_decode(seq2seq_transformer, source, source_mask, max_length, start_token):\n","    seq2seq_transformer.eval()\n","    source = source.to(DEVICE)\n","    source_mask = source_mask.to(DEVICE)\n","    memory = seq2seq_transformer.encode(source, source_mask)\n","    decoded_tokens = torch.ones(1, 1).fill_(start_token).type(torch.long).to(DEVICE)\n","    for i in range(max_length - 1):\n","        target_mask = generate_upper_triangle_mask(decoded_tokens.size(0)).to(DEVICE)\n","        output = seq2seq_transformer.decode(decoded_tokens, memory, target_mask)\n","        output = output.transpose(0, 1)\n","        probabilities = seq2seq_transformer.output_layer(output[:, -1])\n","        _, next_token = torch.max(probabilities, dim=1)\n","        next_token = next_token.item()\n","        decoded_tokens = torch.cat([decoded_tokens, torch.ones(1, 1).type_as(source.data).fill_(next_token)], dim=0)\n","        if next_token == END_IDX:\n","            break\n","    return decoded_tokens\n","def translate(seq2seq_transformer, source_sentence):\n","    seq2seq_transformer.eval()\n","    source = text_transform_map[SOURCE_LANG](source_sentence).view(-1, 1).to(DEVICE)\n","    source_mask = torch.zeros(source.shape[0], source.shape[0]).type(torch.bool).to(DEVICE)\n","    target_tokens = greedy_decode(seq2seq_transformer, source, source_mask, max_length=source.shape[0] + 5, start_token=START_IDX).flatten()\n","    return \" \".join(vocab_map[TARGET_LANG].lookup_tokens(list(target_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"markdown","metadata":{"id":"1J6xK2l36CZC"},"source":["The code develops the functions responsible for the training loop of a sequence to sequence transformer model for a task of machine translation, greedy decoding, as well as translating. The training loop executes a specified number of epochs, measuring time in each of them and calculating the training and validation losses by means of the train_one_epoch and evaluate_model functions, respectively. A decoding technique, which is the greedy_decode function, is implemented in such a way that the model generates the next token one step at a time without forecasting what the next token is. It starts with the start token (START_IDX) and generates tokens until an end Up to when the maximum length of the header and the end token are produced (END_IDX). The translate function takes a source sentence, transforms it to the target space, prepares a source mask for the attention mechanism. It can perform along these lines by applying a greedy – decoding technique to synthesize target tokens, the synthesis is through the application of reconstructing through the inverse of the process applied when generating the syntactic structure. The last translation output will not include the inflection base forms to special tokens, does not include treaters, produces a clean sentence of a target language."]},{"cell_type":"markdown","metadata":{"id":"v33Ah_la6JB7"},"source":["#### Step 3: Insert novel sentences into your English-to-German model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJO9Xn6153h4","outputId":"2d90fea0-6aaa-4771-d5db-d499cb2d8ed1"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Eine Gruppe von Personen steht vor einem Iglu . \n"]}],"source":["print(translate(seq2seq_transformer, \"A group of people standing in front of an igloo\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5C273vd6mEw","outputId":"c6485642-b7e6-4195-e7b7-e9410e69d0ca"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Die Katze ist im Haus . \n"]}],"source":["print(translate(seq2seq_transformer, \"The cat is in the house .\"))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}