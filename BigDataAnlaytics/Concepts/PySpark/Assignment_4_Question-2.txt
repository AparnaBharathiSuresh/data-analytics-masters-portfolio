
C:\spark\spark-3.5.3-bin-hadoop3\bin>more departuredelays.csv
date,delay,distance,origin,destination
01011245,6,602,ABE,ATL
01020600,-8,369,ABE,DTW
01021245,-2,602,ABE,ATL
01020605,-4,602,ABE,ATL
01031245,-4,602,ABE,ATL
01030605,0,602,ABE,ATL
01041243,10,602,ABE,ATL
01040605,28,602,ABE,ATL
01051245,88,602,ABE,ATL
01050605,9,602,ABE,ATL
01061215,-6,602,ABE,ATL
01061725,69,602,ABE,ATL
01061230,0,369,ABE,DTW
01060625,-3,602,ABE,ATL
01070600,0,369,ABE,DTW
01071725,0,602,ABE,ATL
01071230,0,369,ABE,DTW
01070625,0,602,ABE,ATL
01071219,0,569,ABE,ORD
01080600,0,369,ABE,DTW
01081230,33,369,ABE,DTW
01080625,1,602,ABE,ATL
01080607,5,569,ABE,ORD
01081219,54,569,ABE,ORD
01091215,43,602,ABE,ATL
01090600,151,369,ABE,DTW
01091725,0,602,ABE,ATL
01091230,-4,369,ABE,DTW
01090625,8,602,ABE,ATL
01091219,83,569,ABE,ORD
01101215,-5,602,ABE,ATL
01100600,-5,369,ABE,DTW

C:\spark\spark-3.5.3-bin-hadoop3\bin>pyspark
Python 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/11/09 09:33:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.3
      /_/

Using Python version 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024 03:06:41)
Spark context Web UI available at http://host.docker.internal:4041
Spark context available as 'sc' (master = local[*], app id = local-1731173635464).
SparkSession available as 'spark'.
>>> from pyspark.sql.types import *
>>> sch = StructType([
...     StructField("date", StringType(), False),
...     StructField("delay", IntegerType(), False),
...     StructField("distance", IntegerType(), False),
...     StructField("origin", StringType(), False),
...     StructField("destination", StringType(), False)
... ])
>>> sch
StructType([StructField('date', StringType(), False), StructField('delay', IntegerType(), False), StructField('distance', IntegerType(), False), StructField('origin', StringType(), False), StructField('destination', StringType(), False)])
>>> df = spark.read.csv('departuredelays.csv', header=True, schema=sch)
>>> df.show(10)
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01011245|    6|     602|   ABE|        ATL|
|01020600|   -8|     369|   ABE|        DTW|
|01021245|   -2|     602|   ABE|        ATL|
|01020605|   -4|     602|   ABE|        ATL|
|01031245|   -4|     602|   ABE|        ATL|
|01030605|    0|     602|   ABE|        ATL|
|01041243|   10|     602|   ABE|        ATL|
|01040605|   28|     602|   ABE|        ATL|
|01051245|   88|     602|   ABE|        ATL|
|01050605|    9|     602|   ABE|        ATL|
+--------+-----+--------+------+-----------+
only showing top 10 rows

>>> df.printSchema()
root
 |-- date: string (nullable = true)
 |-- delay: integer (nullable = true)
 |-- distance: integer (nullable = true)
 |-- origin: string (nullable = true)
 |-- destination: string (nullable = true)

>>> df_sj=df.filter(df["destination"]=="SJC")
>>> df_sj.show(10)
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01011944|   10|    1839|   ATL|        SJC|
|01021944|   40|    1839|   ATL|        SJC|
|01031944|    1|    1839|   ATL|        SJC|
|01041944|   10|    1839|   ATL|        SJC|
|01051944|   50|    1839|   ATL|        SJC|
|01061941|   77|    1839|   ATL|        SJC|
|01081941|   55|    1839|   ATL|        SJC|
|01091941|   25|    1839|   ATL|        SJC|
|01101941|   16|    1839|   ATL|        SJC|
|01121941|   50|    1839|   ATL|        SJC|
+--------+-----+--------+------+-----------+
only showing top 10 rows

>>> from pyspark.sql.functions import avg
>>> df_delay=df.groupBy('origin').agg(avg('delay').alias('avg_delay'))
>>> df_delay.show()
+------+-------------------+
|origin|          avg_delay|
+------+-------------------+
|   BUR|  8.316794644615081|
|   EUG|  7.568056648308419|
|   BTM|-0.7666666666666667|
|   COD|  2.374301675977654|
|   FAR| 15.780968006562757|
|   FSM|  4.881666666666667|
|   DCA|  8.012508036705828|
|   CID|  16.57703927492447|
|   EVV| 11.860088365243005|
|   CRW| 11.121693121693122|
|   CDV| -5.752808988764045|
|   CMH| 14.022459893048127|
|   CAK|  8.965957446808511|
|   CHO|  8.016556291390728|
|   CEC|  11.60655737704918|
|   CVG| 11.397058823529411|
|   BUF|  12.44192439862543|
|   CDC|0.33116883116883117|
|   ACT|  0.897025171624714|
|   AUS| 10.835627368841013|
+------+-------------------+
only showing top 20 rows

>>> path = "C:\\spark\\output"
>>> df_delay.write.format("parquet").save(path)
>>> best_performing = df_delay.orderBy('avg_delay').first()
>>> print(best_performing)
Row(origin='YAK', avg_delay=-6.685393258426966)
>>> worst_performing = df_delay.orderBy('avg_delay', ascending=False).first()
>>> print(worst_performing)
Row(origin='GUM', avg_delay=33.87777777777778)
>>> with open("best_and_worst_airports.txt", "w") as file:
...     file.write(f"Best Performing Airport: {best_performing['origin']} - Avg Delay: {best_performing['avg_delay']}\n")
...     file.write(f"Worst Performing Airport: {worst_performing['origin']} - Avg Delay: {worst_performing['avg_delay']}\n")
...
61
61
>>>
