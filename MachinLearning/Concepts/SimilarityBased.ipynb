{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Define your custom KNN function \n",
    "def my_knn(X_train, y_train, X_test, n_neighbors=3, weights='uniform'):\n",
    "    from collections import Counter\n",
    "\n",
    "    def euclidean_distance(x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def get_neighbors(X_train, y_train, test_sample, k):\n",
    "        distances = []\n",
    "        for i in range(len(X_train)):\n",
    "            dist = euclidean_distance(test_sample, X_train[i])\n",
    "            distances.append((y_train[i], dist))\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        return distances[:k]\n",
    "\n",
    "    y_pred = []\n",
    "    for test_sample in X_test:\n",
    "        neighbors = get_neighbors(X_train, y_train, test_sample, n_neighbors)\n",
    "        if weights == 'uniform':\n",
    "            # Majority voting (uniform weights)\n",
    "            classes = [neighbor[0] for neighbor in neighbors]\n",
    "            vote = Counter(classes).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Weighted voting (inverse distance weights)\n",
    "            class_votes = {}\n",
    "            for neighbor in neighbors:\n",
    "                label, distance = neighbor\n",
    "                weight = 1 / (distance + 1e-5)  # to avoid division by zero\n",
    "                if label in class_votes:\n",
    "                    class_votes[label] += weight\n",
    "                else:\n",
    "                    class_votes[label] = weight\n",
    "            vote = max(class_votes, key=class_votes.get)\n",
    "        y_pred.append(vote)\n",
    "    \n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.31, random_state=42)\n",
    "\n",
    "# Train and evaluate custom KNN\n",
    "y_pred_custom = my_knn(X_train, y_train, X_test, n_neighbors=5)\n",
    "custom_knn_acc = np.mean(y_pred_custom == y_test)\n",
    "\n",
    "# Train and evaluate sklearn KNN for comparison\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sklearn_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "sklearn_knn.fit(X_train, y_train)\n",
    "y_pred_sklearn = sklearn_knn.predict(X_test)\n",
    "sklearn_knn_acc = np.mean(y_pred_sklearn == y_test)\n",
    "\n",
    "print(f\"Custom KNN Accuracy: {custom_knn_acc:.4f}\")\n",
    "print(f\"Sklearn KNN Accuracy: {sklearn_knn_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c3516",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "Both models perform similarly in terms of accuracy, but this is dependent on the dataset, parameters (like number of neighbors), and test data.For small datasets like Iris, the performance difference in terms of accuracy is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cad26",
   "metadata": {},
   "source": [
    "# Complexity\n",
    "\n",
    "Time Complexity:\n",
    "\n",
    "The KNN algorithm has a time complexity of O(nâ‹…d) per query, where:n is the number of training samples,d is the number of features.This holds for both the custom and sklearn versions since KNN requires calculating the distance from every test point to every training point (brute-force approach).\n",
    "\n",
    "The custom version may be slower due to lack of optimizations.\n",
    "\n",
    "Sklearn's KNN implementation will generally outperform a custom brute-force implementation, especially for large datasets.\n",
    "\n",
    "Memory Complexity:\n",
    "\n",
    "Custom KNN:\n",
    "Stores all training data in memory, as KNN is a lazy learner.Depending on the dataset size, memory usage may become inefficient, especially for large datasets.\n",
    "Sklearn KNN:\n",
    "Similar to custom KNN, but optimized storage and possibly lower memory overhead due to efficient internal structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd9e86",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Custom KNN is a good learning exercise and works fine for small datasets. However, it lacks the optimizations required for large-scale problems, which makes it inefficient in terms of time and memory for larger datasets.\n",
    "Sklearn KNN is the better choice for real-world applications due to its optimizations, flexibility, and ease of use. It is faster and more memory-efficient, particularly for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
